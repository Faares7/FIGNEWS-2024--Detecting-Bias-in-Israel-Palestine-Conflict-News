{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knKZwzSB_nGS",
        "outputId": "2f62e319-2251-486a-8591-7b8a8513a4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fasttext) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Building wheels for collected packages: fasttext, langdetect\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp312-cp312-linux_x86_64.whl size=4498215 sha256=0c86c5936816e6bd295a48a9a5a99e0bb56a2473a11642bef53ae83c6cc5f253\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/27/95/a7baf1b435f1cbde017cabdf1e9688526d2b0e929255a359c6\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=eef73009bd50eaacb51a65e60d289bb624ad401632c056a3ccefed99d8f37d1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built fasttext langdetect\n",
            "Installing collected packages: pybind11, langdetect, fasttext\n",
            "Successfully installed fasttext-0.9.3 langdetect-1.0.9 pybind11-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn imbalanced-learn\n",
        "!pip install transformers datasets torch accelerate\n",
        "!pip install fasttext langdetect openpyxl joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBlRVI1ePOVn"
      },
      "source": [
        "# final ensemble perfect model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irsY2D6_nRS-",
        "outputId": "07d8cf7d-c05d-4d08-c6da-407c51b84566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted\n",
            "================================================================================\n",
            "ENSEMBLE INFERENCE PIPELINE (Shared Drive)\n",
            "================================================================================\n",
            "\n",
            "[STEP 1] Loading test data...\n",
            "  Test set: 240 samples\n",
            "\n",
            "================================================================================\n",
            "LOADING ALL MODELS\n",
            "================================================================================\n",
            "\n",
            "[Loading] Arabic Random Forest...\n",
            "  Downloading FastText...\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.bin.gz\n",
            "\n",
            "\n",
            "[Loading] English Random Forest...\n",
            "\n",
            "[Loading] MARBERTv2...\n",
            "  ✓ Loaded successfully from /content/drive/MyDrive/fignews_shared_project/models/marbert_finetuned/\n",
            "\n",
            "[Loading] DeBERTa-v3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fignews_shared_project/models/deberta_finetuned/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Loaded successfully from /content/drive/MyDrive/fignews_shared_project/models/deberta_finetuned/\n",
            "\n",
            "[Loading] XLM-RoBERTa...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fignews_shared_project/models/xlm_roberta_best_model/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Loaded successfully from /content/drive/MyDrive/fignews_shared_project/models/xlm_roberta_best_model/\n",
            "\n",
            "================================================================================\n",
            "SYSTEM 1: ENSEMBLE EVALUATION\n",
            "================================================================================\n",
            "\n",
            "[System 1] Ensemble Voting Inference...\n",
            "\n",
            "Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "   Biased Against Israel     0.0000    0.0000    0.0000         6\n",
            "Biased Against Palestine     0.5417    0.2097    0.3023        62\n",
            "                  Others     0.1500    0.8000    0.2526        15\n",
            "                Unbiased     0.7407    0.6369    0.6849       157\n",
            "\n",
            "                accuracy                         0.5208       240\n",
            "               macro avg     0.3581    0.4117    0.3100       240\n",
            "            weighted avg     0.6339    0.5208    0.5419       240\n",
            "\n",
            "Accuracy: 0.5208, Macro F1: 0.3100\n",
            "\n",
            "================================================================================\n",
            "SYSTEM 2: BASELINE EVALUATION\n",
            "================================================================================\n",
            "\n",
            "[System 2] XLM-R Baseline Inference...\n",
            "\n",
            "Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "   Biased Against Israel     0.0000    0.0000    0.0000         6\n",
            "Biased Against Palestine     0.4762    0.3226    0.3846        62\n",
            "                  Others     1.0000    0.1333    0.2353        15\n",
            "                Unbiased     0.6990    0.8726    0.7762       157\n",
            "\n",
            "                accuracy                         0.6625       240\n",
            "               macro avg     0.5438    0.3321    0.3490       240\n",
            "            weighted avg     0.6428    0.6625    0.6218       240\n",
            "\n",
            "Accuracy: 0.6625, Macro F1: 0.3490\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "FIGNEWS-2024: ENSEMBLE INFERENCE (TEAM VERSION - SHARED DRIVE) - FIXED\n",
        "======================================================================\n",
        "- Fixes 'AttributeError: str object has no attribute re'\n",
        "- Fixes DeBERTa loading path (updated to 'deberta_finetuned1')\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# GOOGLE DRIVE MOUNT\n",
        "# ============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"✓ Google Drive mounted\")\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import os\n",
        "import warnings\n",
        "import re\n",
        "import string\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from langdetect import detect\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ENSEMBLE INFERENCE PIPELINE (Shared Drive)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Pipeline configuration\"\"\"\n",
        "\n",
        "    # ========== SHARED DRIVE PATHS ==========\n",
        "    BASE_PATH = \"/content/drive/MyDrive/fignews_shared_project/\"\n",
        "\n",
        "    # Data paths\n",
        "    MAIN_FILE = BASE_PATH + \"data/Main.xlsx\"\n",
        "    IAA_FILES = [\n",
        "        BASE_PATH + \"data/IAA-1.xlsx\",\n",
        "        BASE_PATH + \"data/IAA-2.xlsx\",\n",
        "        BASE_PATH + \"data/IAA-3.xlsx\",\n",
        "        BASE_PATH + \"data/IAA-4.xlsx\"\n",
        "    ]\n",
        "\n",
        "    # Model directories\n",
        "    # UPDATED paths based on your training logs\n",
        "    CLASSICAL_DIR = BASE_PATH + \"models/classical/\"\n",
        "    MARBERT_DIR = BASE_PATH + \"models/marbert_finetuned/\"\n",
        "    # Note: Updated to 'deberta_finetuned1' to match your training output\n",
        "    DEBERTA_DIR = BASE_PATH + \"models/deberta_finetuned/\"\n",
        "    XLMR_DIR = BASE_PATH + \"models/xlm_roberta_best_model/\" # Updated based on your log error\n",
        "\n",
        "    # Fallback if XLM path is different:\n",
        "    if not os.path.exists(XLMR_DIR):\n",
        "        XLMR_DIR = BASE_PATH + \"models/xlmr_finetuned/\"\n",
        "\n",
        "    # Labels\n",
        "    LABEL_MAP = {\n",
        "        'Unbiased': 'Unbiased',\n",
        "        'Biased against Palestine': 'Biased Against Palestine',\n",
        "        'Biased Against Palestine': 'Biased Against Palestine',\n",
        "        'Biased against Israel': 'Biased Against Israel',\n",
        "        'Biased Against Israel': 'Biased Against Israel',\n",
        "        'Unclear': 'Others',\n",
        "        'Biased against others': 'Others',\n",
        "        'Biased against both': 'Others',\n",
        "        'Biased against both Palestine and Israel': 'Others',\n",
        "        'Not Applicable': 'Others',\n",
        "        'Others': 'Others'\n",
        "    }\n",
        "\n",
        "    TARGET_LABELS = ['Unbiased', 'Biased Against Palestine',\n",
        "                     'Biased Against Israel', 'Others']\n",
        "    LABEL2ID = {label: idx for idx, label in enumerate(TARGET_LABELS)}\n",
        "    ID2LABEL = {idx: label for label, idx in LABEL2ID.items()}\n",
        "\n",
        "    FASTTEXT_AR_MODEL = \"cc.ar.300.bin\"\n",
        "    FASTTEXT_DIM = 300\n",
        "\n",
        "    IAA_TRAIN_SPLIT = 0.8\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PREPROCESSING (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_classical_arabic(text: str) -> str:\n",
        "    \"\"\"Preprocessing for Arabic Classical Model.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text).replace(':=:', ' ')\n",
        "    text = re.sub(r'[a-zA-Z]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    arabic_punctuation = '،؛؟!()[]{}\"\"\"\\'\\'`'\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation + arabic_punctuation))\n",
        "    text = re.sub(r'[إأآا]', 'ا', text).replace('ى', 'ي')\n",
        "    text = text.replace('ة', 'ه').replace('ئ', 'ي')\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "def preprocess_classical_english(text: str) -> str:\n",
        "    \"\"\"Preprocessing for English Classical Model.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    # Fixed the regex chaining error here\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text).replace(':=:', ' ')\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "def clean_urls_and_format(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return re.sub(r'\\s+', ' ', re.sub(r'http\\S+|www\\.\\S+', '', text).replace(':=:', ' ')).strip()\n",
        "\n",
        "\n",
        "def filter_valid_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    check_cols = [c for c in ['Text', 'Arabic MT', 'English MT'] if c in df.columns]\n",
        "    if not check_cols: return df\n",
        "    mask = df[check_cols].apply(lambda x: x.astype(str).str.strip().str.len() > 0).any(axis=1)\n",
        "    return df[mask].copy()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_clean_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    print(\"\\n[STEP 1] Loading test data...\")\n",
        "    if not os.path.exists(Config.MAIN_FILE):\n",
        "        raise FileNotFoundError(f\"{Config.MAIN_FILE} not found!\")\n",
        "\n",
        "    main_df = pd.read_excel(Config.MAIN_FILE)\n",
        "    main_df = main_df[main_df['Bias'].notna() & (main_df['Bias'] != '')]\n",
        "    main_df['Bias'] = main_df['Bias'].astype(str).str.strip()\n",
        "    for col in ['Text', 'Arabic MT', 'English MT']:\n",
        "        if col in main_df.columns: main_df[col] = main_df[col].apply(clean_urls_and_format)\n",
        "    main_df = filter_valid_data(main_df)\n",
        "\n",
        "    iaa_dfs = []\n",
        "    for f in Config.IAA_FILES:\n",
        "        if os.path.exists(f):\n",
        "            t = pd.read_excel(f)\n",
        "            if 'Bais' in t.columns: t['Bias'] = t['Bais']\n",
        "            t = t[t['Bias'].notna() & (t['Bias'] != '')]\n",
        "            t['Bias'] = t['Bias'].astype(str).str.strip()\n",
        "            for c in ['Text', 'Arabic MT', 'English MT']:\n",
        "                if c in t.columns: t[c] = t[c].apply(clean_urls_and_format)\n",
        "            iaa_dfs.append(filter_valid_data(t))\n",
        "    iaa_df = pd.concat(iaa_dfs, ignore_index=True) if iaa_dfs else pd.DataFrame()\n",
        "    return main_df, iaa_df\n",
        "\n",
        "\n",
        "def map_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df['Bias_Mapped'] = df['Bias'].map(Config.LABEL_MAP).fillna('Others')\n",
        "    return df\n",
        "\n",
        "\n",
        "def apply_majority_vote(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['Text_ID'] = df['ID'].astype(str) + \"_\" + df['Text'].str[:20]\n",
        "    gold_rows = []\n",
        "    for _, g in df.groupby('Text_ID'):\n",
        "        maj = Counter(g['Bias_Mapped']).most_common(1)[0][0]\n",
        "        r = g.iloc[0].copy()\n",
        "        r['Bias_Mapped'] = maj\n",
        "        gold_rows.append(r)\n",
        "    return pd.DataFrame(gold_rows)\n",
        "\n",
        "\n",
        "def get_test_set(main_df, iaa_df):\n",
        "    u_ids = (iaa_df['Text_ID'].unique() if 'Text_ID' in iaa_df.columns else iaa_df['ID'].unique())\n",
        "    _, test_ids = train_test_split(u_ids, test_size=(1 - Config.IAA_TRAIN_SPLIT), random_state=Config.RANDOM_STATE)\n",
        "\n",
        "    if 'Text_ID' in iaa_df.columns:\n",
        "        iaa_test = iaa_df[iaa_df['Text_ID'].isin(test_ids)].copy()\n",
        "    else:\n",
        "        iaa_test = iaa_df[iaa_df['ID'].isin(test_ids)].copy()\n",
        "    return apply_majority_vote(iaa_test)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL LOADERS\n",
        "# ============================================================================\n",
        "\n",
        "class ArabicRFLoader:\n",
        "    def __init__(self, model_dir):\n",
        "        print(\"\\n[Loading] Arabic Random Forest...\")\n",
        "        path = os.path.join(model_dir, 'rf_arabic.pkl')\n",
        "        if not os.path.exists(path): raise FileNotFoundError(f\"Missing {path}\")\n",
        "        self.rf = joblib.load(path)\n",
        "\n",
        "        if not os.path.exists(Config.FASTTEXT_AR_MODEL):\n",
        "             print(\"  Downloading FastText...\")\n",
        "             fasttext.util.download_model('ar', if_exists='ignore')\n",
        "        self.ft = fasttext.load_model(Config.FASTTEXT_AR_MODEL)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        X = np.array([self._vec(t) for t in texts])\n",
        "        return [Config.ID2LABEL[p] for p in self.rf.predict(X)]\n",
        "\n",
        "    def _vec(self, text):\n",
        "        if not isinstance(text, str) or not text.strip(): return np.zeros(300)\n",
        "        w = text.split()\n",
        "        v = [self.ft.get_word_vector(x) for x in w if x.strip()]\n",
        "        return np.mean(v, axis=0) if v else np.zeros(300)\n",
        "\n",
        "class EnglishRFLoader:\n",
        "    def __init__(self, model_dir):\n",
        "        print(\"\\n[Loading] English Random Forest...\")\n",
        "        m_path = os.path.join(model_dir, 'rf_english.pkl')\n",
        "        v_path = os.path.join(model_dir, 'tfidf_english.pkl')\n",
        "        if not os.path.exists(m_path): raise FileNotFoundError(f\"Missing {m_path}\")\n",
        "        self.rf = joblib.load(m_path)\n",
        "        self.vec = joblib.load(v_path)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        X = self.vec.transform(texts)\n",
        "        return [Config.ID2LABEL[p] for p in self.rf.predict(X)]\n",
        "\n",
        "class TransformerLoader:\n",
        "    def __init__(self, model_dir, name):\n",
        "        print(f\"\\n[Loading] {name}...\")\n",
        "\n",
        "        # Robust path checking\n",
        "        if model_dir is None or not os.path.exists(model_dir):\n",
        "            raise FileNotFoundError(f\"Missing model directory: {model_dir}\")\n",
        "\n",
        "        try:\n",
        "            self.tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "            self.mod = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.mod.to(self.device)\n",
        "            self.mod.eval()\n",
        "            print(f\"  ✓ Loaded successfully from {model_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Failed to load {name}: {e}\")\n",
        "            raise e\n",
        "\n",
        "    def predict(self, texts):\n",
        "        if not texts: return []\n",
        "        inp = self.tok(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out = self.mod(**inp)\n",
        "            preds = torch.argmax(out.logits, dim=-1)\n",
        "        return [Config.ID2LABEL[p.item()] for p in preds]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ENSEMBLE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "class EnsemblePipeline:\n",
        "    def __init__(self):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LOADING ALL MODELS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.arabic_rf = None\n",
        "        self.english_rf = None\n",
        "        self.marbert = None\n",
        "        self.deberta = None\n",
        "        self.xlmr = None\n",
        "\n",
        "        # Load Safely\n",
        "        try: self.arabic_rf = ArabicRFLoader(Config.CLASSICAL_DIR)\n",
        "        except Exception as e: print(f\"  Warning: Arabic RF failed: {e}\")\n",
        "\n",
        "        try: self.english_rf = EnglishRFLoader(Config.CLASSICAL_DIR)\n",
        "        except Exception as e: print(f\"  Warning: English RF failed: {e}\")\n",
        "\n",
        "        try: self.marbert = TransformerLoader(Config.MARBERT_DIR, \"MARBERTv2\")\n",
        "        except Exception as e: print(f\"  Warning: MARBERT failed: {e}\")\n",
        "\n",
        "        try: self.deberta = TransformerLoader(Config.DEBERTA_DIR, \"DeBERTa-v3\")\n",
        "        except Exception as e: print(f\"  Warning: DeBERTa failed: {e}\")\n",
        "\n",
        "        try: self.xlmr = TransformerLoader(Config.XLMR_DIR, \"XLM-RoBERTa\")\n",
        "        except Exception as e: print(f\"  Warning: XLM-R failed: {e}\")\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        try:\n",
        "            lang = detect(text)\n",
        "            return 'ar' if lang == 'ar' else 'en'\n",
        "        except:\n",
        "            return 'en'\n",
        "\n",
        "    def predict_system1(self, df: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"System 1: Route to Language Specialist + Vote (Updated with RF Tie-Breaker).\"\"\"\n",
        "        print(\"\\n[System 1] Ensemble Voting Inference...\")\n",
        "        predictions = []\n",
        "        texts = df['Text'].tolist()\n",
        "\n",
        "        texts_ar_clean = [preprocess_classical_arabic(t) for t in texts]\n",
        "        texts_en_clean = [preprocess_classical_english(t) for t in texts]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            lang = self.detect_language(text)\n",
        "            votes = []\n",
        "            rf_vote = None  # NEW: Track the RF vote specifically\n",
        "\n",
        "            # --- ARABIC ROUTE ---\n",
        "            if lang == 'ar':\n",
        "                if self.marbert:\n",
        "                    votes.append(self.marbert.predict([text])[0])\n",
        "                if self.arabic_rf:\n",
        "                    p = self.arabic_rf.predict([texts_ar_clean[i]])[0]\n",
        "                    votes.append(p)\n",
        "                    rf_vote = p  # Capture RF decision\n",
        "                if self.xlmr:\n",
        "                    votes.append(self.xlmr.predict([text])[0])\n",
        "\n",
        "            # --- ENGLISH ROUTE ---\n",
        "            else:\n",
        "                if self.deberta:\n",
        "                    votes.append(self.deberta.predict([text])[0])\n",
        "                if self.english_rf:\n",
        "                    p = self.english_rf.predict([texts_en_clean[i]])[0]\n",
        "                    votes.append(p)\n",
        "                    rf_vote = p  # Capture RF decision\n",
        "                if self.xlmr:\n",
        "                    votes.append(self.xlmr.predict([text])[0])\n",
        "\n",
        "            # --- FALLBACK ---\n",
        "            if not votes:\n",
        "                votes = [\"Others\"]\n",
        "\n",
        "            # Vote Counting\n",
        "            vote_counts = Counter(votes)\n",
        "            most_common = vote_counts.most_common()\n",
        "\n",
        "            if len(most_common) == 1:\n",
        "                predictions.append(most_common[0][0])\n",
        "            elif most_common[0][1] > most_common[1][1]:\n",
        "                predictions.append(most_common[0][0])\n",
        "            else:\n",
        "                # --- TIE BREAKER CHANGED HERE ---\n",
        "                # Old logic: predictions.append(llm_vote if llm_vote else most_common[0][0])\n",
        "                # New logic: Priority to Random Forest (Highest Accuracy)\n",
        "                if rf_vote:\n",
        "                    predictions.append(rf_vote)\n",
        "                else:\n",
        "                    # Fallback if RF somehow failed to run\n",
        "                    predictions.append(most_common[0][0])\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_system2(self, df: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"System 2: Baseline XLM-R only.\"\"\"\n",
        "        print(\"\\n[System 2] XLM-R Baseline Inference...\")\n",
        "        if not self.xlmr:\n",
        "            return [\"Others\"] * len(df)\n",
        "        return self.xlmr.predict(df['Text'].tolist())\n",
        "\n",
        "    def evaluate(self, test_df: pd.DataFrame, system='system1'):\n",
        "        true_labels = test_df['Bias_Mapped'].tolist()\n",
        "\n",
        "        if system == 'system1':\n",
        "            predictions = self.predict_system1(test_df)\n",
        "        else:\n",
        "            predictions = self.predict_system2(test_df)\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(true_labels, predictions, digits=4, zero_division=0))\n",
        "\n",
        "        acc = accuracy_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
        "        print(f\"Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    np.random.seed(Config.RANDOM_STATE)\n",
        "\n",
        "    try: main_df, iaa_df = load_and_clean_data()\n",
        "    except FileNotFoundError: return\n",
        "\n",
        "    main_df = map_labels(main_df)\n",
        "    iaa_df = map_labels(iaa_df) if len(iaa_df) > 0 else iaa_df\n",
        "\n",
        "    # Get Test Set\n",
        "    test_df = get_test_set(main_df, iaa_df)\n",
        "    print(f\"  Test set: {len(test_df)} samples\")\n",
        "\n",
        "    # Run Inference\n",
        "    pipeline = EnsemblePipeline()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYSTEM 1: ENSEMBLE EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "    pipeline.evaluate(test_df, system='system1')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYSTEM 2: BASELINE EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "    pipeline.evaluate(test_df, system='system2')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmmw-CWbPKPO"
      },
      "source": [
        "# Testing arabic and english ensemble model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01APa3T9oQC8",
        "outputId": "85fe8368-f265-4cf9-f36f-6941362e5a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Pipeline...\n",
            "\n",
            "================================================================================\n",
            "LOADING ALL MODELS\n",
            "================================================================================\n",
            "\n",
            "[Loading] Arabic Random Forest...\n",
            "\n",
            "[Loading] English Random Forest...\n",
            "\n",
            "[Loading] MARBERTv2...\n",
            "  ✓ Loaded successfully from /content/drive/MyDrive/fignews_shared_project/models/marbert_finetuned/\n",
            "\n",
            "[Loading] DeBERTa-v3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fignews_shared_project/models/deberta_finetuned/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Loaded successfully from /content/drive/MyDrive/fignews_shared_project/models/deberta_finetuned/\n",
            "\n",
            "[Loading] XLM-RoBERTa...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fignews_shared_project/models/xlm_roberta_best_model/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Loaded successfully from /content/drive/MyDrive/fignews_shared_project/models/xlm_roberta_best_model/\n",
            "------------------------------------------------------------\n",
            "\n",
            "Scanning Text (ar): 'قوات الاحتلال ارتكبت مجزرة مروعة بحق المدنيين العزل في القطاع'\n",
            "   Individual Votes: {'MARBERT': 'Biased Against Israel', 'XLM-R': 'Unbiased', 'RandomForest_AR': 'Unbiased'}\n",
            "   Result: Majority Vote (Unbiased)\n",
            "FINAL ARABIC PREDICTION: Unbiased\n",
            "------------------------------------------------------------\n",
            "\n",
            "Scanning Text (en): 'The barbaric actions of the regime show they have no regard for human life.'\n",
            "   Individual Votes: {'DeBERTa': 'Biased Against Palestine', 'XLM-R': 'Unbiased', 'RandomForest_EN': 'Unbiased'}\n",
            "   Result: Majority Vote (Unbiased)\n",
            "FINAL ENGLISH PREDICTION: Unbiased\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM INFERENCE WITH RF TIE-BREAKER\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Ensure Pipeline is Loaded\n",
        "if 'pipeline' not in locals():\n",
        "    print(\"Initializing Pipeline...\")\n",
        "    pipeline = EnsemblePipeline()\n",
        "else:\n",
        "    print(\"Using existing Pipeline instance.\")\n",
        "\n",
        "def predict_with_rf_priority(text: str, pipeline_obj):\n",
        "    \"\"\"\n",
        "    Predicts bias with a specific tie-breaker:\n",
        "    If votes are tied, default to the Random Forest model (Highest Accuracy).\n",
        "    \"\"\"\n",
        "    # Detect Language\n",
        "    lang = pipeline_obj.detect_language(text)\n",
        "    votes = {}\n",
        "    rf_prediction = None\n",
        "\n",
        "    print(f\"\\nScanning Text ({lang}): '{text}'\")\n",
        "\n",
        "    # --- ARABIC LOGIC ---\n",
        "    if lang == 'ar':\n",
        "        clean_text = preprocess_classical_arabic(text)\n",
        "\n",
        "        # 1. MARBERT\n",
        "        if pipeline_obj.marbert:\n",
        "            pred = pipeline_obj.marbert.predict([text])[0]\n",
        "            votes['MARBERT'] = pred\n",
        "\n",
        "        # 2. XLM-R\n",
        "        if pipeline_obj.xlmr:\n",
        "            pred = pipeline_obj.xlmr.predict([text])[0]\n",
        "            votes['XLM-R'] = pred\n",
        "\n",
        "        # 3. Random Forest (The Tie-Breaker)\n",
        "        if pipeline_obj.arabic_rf:\n",
        "            pred = pipeline_obj.arabic_rf.predict([clean_text])[0]\n",
        "            votes['RandomForest_AR'] = pred\n",
        "            rf_prediction = pred\n",
        "\n",
        "    # --- ENGLISH LOGIC ---\n",
        "    else:\n",
        "        clean_text = preprocess_classical_english(text)\n",
        "\n",
        "        # 1. DeBERTa\n",
        "        if pipeline_obj.deberta:\n",
        "            pred = pipeline_obj.deberta.predict([text])[0]\n",
        "            votes['DeBERTa'] = pred\n",
        "\n",
        "        # 2. XLM-R\n",
        "        if pipeline_obj.xlmr:\n",
        "            pred = pipeline_obj.xlmr.predict([text])[0]\n",
        "            votes['XLM-R'] = pred\n",
        "\n",
        "        # 3. Random Forest (The Tie-Breaker)\n",
        "        if pipeline_obj.english_rf:\n",
        "            pred = pipeline_obj.english_rf.predict([clean_text])[0]\n",
        "            votes['RandomForest_EN'] = pred\n",
        "            rf_prediction = pred\n",
        "\n",
        "    # --- VOTING LOGIC ---\n",
        "    vote_list = list(votes.values())\n",
        "    if not vote_list:\n",
        "        return \"Others\"\n",
        "\n",
        "    counts = Counter(vote_list)\n",
        "    most_common = counts.most_common()\n",
        "\n",
        "    print(f\"   Individual Votes: {votes}\")\n",
        "\n",
        "    # DECISION TREE\n",
        "    final_pred = \"\"\n",
        "\n",
        "    # Case A: Unanimous\n",
        "    if len(most_common) == 1:\n",
        "        final_pred = most_common[0][0]\n",
        "        print(f\"   Result: Unanimous ({final_pred})\")\n",
        "\n",
        "    # Case B: Clear Majority\n",
        "    elif most_common[0][1] > most_common[1][1]:\n",
        "        final_pred = most_common[0][0]\n",
        "        print(f\"   Result: Majority Vote ({final_pred})\")\n",
        "\n",
        "    # Case C: Tie -> Use RF\n",
        "    else:\n",
        "        if rf_prediction:\n",
        "            final_pred = rf_prediction\n",
        "            print(f\"   Result: TIE DETECTED -> Using Highest Acc Model (RF): {final_pred}\")\n",
        "        else:\n",
        "            # Fallback if RF is missing for some reason\n",
        "            final_pred = most_common[0][0]\n",
        "            print(f\"   Result: Tie (RF missing, using first option): {final_pred}\")\n",
        "\n",
        "    return final_pred\n",
        "\n",
        "# ============================================================================\n",
        "# TEST INPUTS\n",
        "# ============================================================================\n",
        "\n",
        "# Input 1: Arabic Sentence (Biased example)\n",
        "# Transl: \"The occupation forces committed a massacre against civilians.\"\n",
        "arabic_text = \"قوات الاحتلال ارتكبت مجزرة مروعة بحق المدنيين العزل في القطاع\"\n",
        "\n",
        "# Input 2: English Sentence (Biased example)\n",
        "english_text = \"The barbaric actions of the regime show they have no regard for human life.\"\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"-\" * 60)\n",
        "ar_result = predict_with_rf_priority(arabic_text, pipeline)\n",
        "print(f\"FINAL ARABIC PREDICTION: {ar_result}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "en_result = predict_with_rf_priority(english_text, pipeline)\n",
        "print(f\"FINAL ENGLISH PREDICTION: {en_result}\")\n",
        "print(\"-\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02adf9c5ea414d3d844e5b264dad6724": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c400a5df0ed4907bc978fbea62cd598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce15792fca148eda46e17955389c803",
            "placeholder": "​",
            "style": "IPY_MODEL_193de67b596e4c57b7c64a3a07824776",
            "value": "Map: 100%"
          }
        },
        "193de67b596e4c57b7c64a3a07824776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce15792fca148eda46e17955389c803": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2219587102284c03b7a17449a9d966d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d973ad205704896be97f44ca0f97c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30b47045104a4dfda568bf89e8623c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c400a5df0ed4907bc978fbea62cd598",
              "IPY_MODEL_70fcea0c0d3049a2a97848fb58b4e4e6",
              "IPY_MODEL_e65a444dfc7042f5b55ed3d49fa330ac"
            ],
            "layout": "IPY_MODEL_02adf9c5ea414d3d844e5b264dad6724"
          }
        },
        "49dc357ec52b40b0a99a2a3a96596f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8687535592fb4ceb8d32fc0662689b62",
            "max": 96,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed68588daa134f9f874b689fd04c9416",
            "value": 96
          }
        },
        "70fcea0c0d3049a2a97848fb58b4e4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d8477dcda1f4e32900f571b2cbdba21",
            "max": 4704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a32195f1ee8542ffa955a1e02c18b9dd",
            "value": 4704
          }
        },
        "71502e68d60540fdbb907831e97da3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d8477dcda1f4e32900f571b2cbdba21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8687535592fb4ceb8d32fc0662689b62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90db3341b7a746a7bf89d868a3bd8ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32195f1ee8542ffa955a1e02c18b9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba32a2d04c044722bd82906fd8bc91c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcea58182a7f4e10a01757a2823a56af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71502e68d60540fdbb907831e97da3ae",
            "placeholder": "​",
            "style": "IPY_MODEL_2219587102284c03b7a17449a9d966d6",
            "value": "Map: 100%"
          }
        },
        "bfb3d2e3668c4b6b8228e4f4c22553d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba32a2d04c044722bd82906fd8bc91c8",
            "placeholder": "​",
            "style": "IPY_MODEL_2d973ad205704896be97f44ca0f97c37",
            "value": " 96/96 [00:00&lt;00:00, 1932.66 examples/s]"
          }
        },
        "e491f00ae2484b9eae62371d70429778": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcea58182a7f4e10a01757a2823a56af",
              "IPY_MODEL_49dc357ec52b40b0a99a2a3a96596f83",
              "IPY_MODEL_bfb3d2e3668c4b6b8228e4f4c22553d9"
            ],
            "layout": "IPY_MODEL_f349f1fa77bc44daa5dd3fe8215af87a"
          }
        },
        "e65a444dfc7042f5b55ed3d49fa330ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90db3341b7a746a7bf89d868a3bd8ec9",
            "placeholder": "​",
            "style": "IPY_MODEL_f1aa9386a0d7491597fff62ef721b5bc",
            "value": " 4704/4704 [00:01&lt;00:00, 4087.28 examples/s]"
          }
        },
        "ed68588daa134f9f874b689fd04c9416": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1aa9386a0d7491597fff62ef721b5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f349f1fa77bc44daa5dd3fe8215af87a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
